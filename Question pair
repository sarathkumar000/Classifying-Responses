df_train = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')
df_test = pd.read_csv('/kaggle/input/quora-question-pairs/test.csv')
df_train.head()


import pandas as pd
import io
!pip install emoji
#!pip install pyspellchecker
import pandas as pd
import numpy as np
import re
import string
import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import wordnet
from nltk.corpus import stopwords 
from nltk.stem.wordnet import WordNetLemmatizer
#from spellchecker import SpellChecker
from emoji.unicode_codes import UNICODE_EMOJI


contraction_mapping = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have", "couldn't": "could not",
                       "didn't": "did not",  "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not", 
                       "he'd": "he would","he'll": "he will", "he's": "he is", "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", 
                       "how's": "how is",  "I'd": "I would", "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have","I'm": "I am", 
                       "I've": "I have", "i'd": "i would", "i'd've": "i would have", "i'll": "i will",  "i'll've": "i will have","i'm": "i am", 
                       "i've": "i have", "isn't": "is not", "it'd": "it would", "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have",
                       "it's": "it is", "let's": "let us", "ma'am": "madam", "mayn't": "may not", "might've": "might have","mightn't": "might not",
                       "mightn't've": "might not have", "must've": "must have", "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", 
                       "needn't've": "need not have","o'clock": "of the clock", "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not", 
                       "sha'n't": "shall not", "shan't've": "shall not have", "she'd": "she would", "she'd've": "she would have", "she'll": "she will", 
                       "she'll've": "she will have", "she's": "she is", "should've": "should have", "shouldn't": "should not", "shouldn't've": "should not have", 
                       "so've": "so have","so's": "so as", "this's": "this is","that'd": "that would", "that'd've": "that would have", "that's": "that is", 
                       "there'd": "there would", "there'd've": "there would have", "there's": "there is", "here's": "here is","they'd": "they would", 
                       "they'd've": "they would have", "they'll": "they will", "they'll've": "they will have", "they're": "they are", "they've": "they have", 
                       "to've": "to have", "wasn't": "was not", "we'd": "we would", "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", 
                       "we're": "we are", "we've": "we have", "weren't": "were not", "what'll": "what will", "what'll've": "what will have", "what're": "what are",  
                       "what's": "what is", "what've": "what have", "when's": "when is", "when've": "when have", "where'd": "where did", "where's": "where is", 
                       "where've": "where have", "who'll": "who will", "who'll've": "who will have", "who's": "who is", "who've": "who have", "why's": "why is", 
                       "why've": "why have", "will've": "will have", "won't": "will not", "won't've": "will not have", "would've": "would have", "wouldn't": "would not", 
                       "wouldn't've": "would not have", "y'all": "you all", "y'all'd": "you all would","y'all'd've": "you all would have",
                       "y'all're": "you all are","y'all've": "you all have","you'd": "you would", "you'd've": "you would have", "you'll": "you will", 
                       "you'll've": "you will have", "you're": "you are", "you've": "you have" }

dic = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 
                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 
                'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 
                'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 
                'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', "mastrubating": 'masturbating', 
                'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 
                'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', "whst": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 
                'demonitization': 'demonetization', 'demonetisation': 'demonetization'}

CHAT_WORDS = """
AFAIK=As Far As I Know
AFK=Away From Keyboard
ASAP=As Soon As Possible
ATK=At The Keyboard
ATM=At The Moment
A3=Anytime, Anywhere, Anyplace
BAK=Back At Keyboard
BBL=Be Back Later
BBS=Be Back Soon
BFN=Bye For Now
B4N=Bye For Now
BRB=Be Right Back
BRT=Be Right There
BTW=By The Way
B4=Before
B4N=Bye For Now
CU=See You
CUL8R=See You Later
CYA=See You
FAQ=Frequently Asked Questions
FC=Fingers Crossed
FWIW=For What It's Worth
FYI=For Your Information
GAL=Get A Life
GG=Good Game
GN=Good Night
GMTA=Great Minds Think Alike
GR8=Great!
G9=Genius
IC=I See
ICQ=I Seek you (also a chat program)
ILU=ILU: I Love You
IMHO=In My Honest/Humble Opinion
IMO=In My Opinion
IOW=In Other Words
IRL=In Real Life
KISS=Keep It Simple, Stupid
LDR=Long Distance Relationship
LMAO=Laugh My A.. Off
LOL=Laughing Out Loud
LTNS=Long Time No See
L8R=Later
MTE=My Thoughts Exactly
M8=Mate
NRN=No Reply Necessary
OIC=Oh I See
PITA=Pain In The A..
PRT=Party
PRW=Parents Are Watching
ROFL=Rolling On The Floor Laughing
ROFLOL=Rolling On The Floor Laughing Out Loud
ROTFLMAO=Rolling On The Floor Laughing My A.. Off
SK8=Skate
STATS=Your sex and age
ASL=Age, Sex, Location
THX=Thank You
TTFN=Ta-Ta For Now!
TTYL=Talk To You Later
U=You
U2=You Too
U4E=Yours For Ever
WB=Welcome Back
WTF=What The F...
WTG=Way To Go!
WUF=Where Are You From?
W8=Wait...
7K=Sick:-D Laugher
"""
chat_words_map_dict = {}
chat_words_list = []
for line in CHAT_WORDS.split("\n"):
      if line != "":
          cw = line.split("=")[0]
          cw_expanded = line.split("=")[1]
          chat_words_list.append(cw)
          chat_words_map_dict[cw] = cw_expanded
chat_words_list = set(chat_words_list)



def frameWork(text):
    return removeUrls(text)

# Method:1 Here We are removing URLS 
def removeUrls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    text=url_pattern.sub(r'', str(text))
    return removeHtmlTags(text)
# Method:2 Here we are removing Html Tags
def removeHtmlTags(text):
    html_pattern = re.compile('<.*?>')
    text=html_pattern.sub(r'', text)
    return removeAndReplaceEmojiEmoti(text)
#from spellchecker import SpellChecker
# Method:3 Here We are removing and replacing emojis
def removeAndReplaceEmojiEmoti(inputString):
    text = ""
    for character in inputString:
        try:
            character.encode("ascii")
            text += character
        except UnicodeEncodeError:
            replaced = unidecode(character)
            if replaced != '':
                text += replaced
            else:
                try:
                     text += unicodedata.name(character) 
                except ValueError:
                     text += "[x]"
    """for emot in EMOTICONS:
        text = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), text)"""
    
  #text=emoticon_pattern.sub(r'',text)
  
    return clean_contractions(text)
#Method:4 Clearing contractions
def clean_contractions(text):
    specials = ["’", "‘", "´", "`"]
    for s in specials:
        text = text.replace(s, "'")
    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(" ")])
  
    return chatWordsConversion(text)
#Method:5 Chat words Conversion
def chatWordsConversion(text):
    new_text = []
    for w in text.split():
        if w.upper() in chat_words_list:
            new_text.append(chat_words_map_dict[w.upper()])
        else:
            new_text.append(w)
    text=" ".join(new_text)
  
    return replaceRepetitionPunc(text)
#Method 6: We are replacing repetitive punctuation
def replaceRepetitionPunc(text):
    """ Replaces repetitions of exlamation marks """
    text = re.sub(r"(\!)\1+", '', text)
    
    """ Replaces repetitions of question marks """
    text = re.sub(r"(\?)\1+", '', text)
    
    """ Replaces repetitions of stop marks """
    text = re.sub(r"(\.)\1+", '', text)
  
    return dealSpecialChars(text)
#Method:7 Here We are dealing with special Characters
def dealSpecialChars(text):
    punct = "/-'?!.,#$%\'()*+-/:;<=>@[\\]^_`{|}~" + '""“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\×™√²—–&'
    mapping = {"‘": "'", "₹": "e", "´": "'", "°": "", "€": "e", "™": "tm", "√": " sqrt ", 
                   "×": "x", "²": "2", "—": "-", "–": "-", "’": "'", "_": "-", "`": "'", '“': '"', '”': '"', '“': '"', "£": "e", '∞': 'infinity', 
                   'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }
  #  We use a map to replace unknown characters with known ones.
    for p in mapping:
        text = text.replace(p, mapping[p])
  #We use a map to replace unknown characters with known ones.

  # We make sure there are spaces between words and punctuation
    
    for p in punct:
        text = text.replace(p, f' {p} ')
  
    return removePunc(text)
#Method:8 Here We are Removing Punctuation
def removePunc(text):
    translator = str.maketrans('', '', string.punctuation)
    text=text.translate(translator)
  
    return tokenize1(text)


#Method:9 From Here Onwards Replacing Negations with Antonyms

def tokenize1(text):
    tokens = nltk.word_tokenize(text)
    tokens = replaceNegations(tokens)
    text = " ".join(tokens)
  
    return removeNum(text)
def replaceNegations(text):
    """ Finds "not" and antonym for the next word and if found, replaces not and the next word with the antonym """
    i, l = 0, len(text)
    words = []
    while i < l:
        word = text[i]
        if word == 'not' and i+1 < l:
            ant = replace(text[i+1])
            if ant:
                words.append(ant)
                i += 2
                continue
        words.append(word)
        i += 1
    return words
def replace(word, pos=None):
    """ Creates a set of all antonyms for the word and if there is only one antonym, it returns it """
    antonyms = set()
    for syn in wordnet.synsets(word, pos=pos):
        for lemma in syn.lemmas():
            for antonym in lemma.antonyms():
                antonyms.add(antonym.name())
    if len(antonyms) == 1:
        return antonyms.pop()
    else:
        return None

#Method:10 this method is for removing Numbers
def removeNum(text):
    text = ''.join([i for i in text if not i.isdigit()]) 
          
    return lower(text)

#Method:11 Here we are converting string to lower
def lower(text):
    text=text.lower()
  
    return tokenize(text)

#Method:12 From Here Replacing Elongated Words superrrrrrrrrrr -> super 
def replaceElongated(word):
    """ Replaces an elongated word with its basic form, unless the word exists in the lexicon """

    repeat_regexp = re.compile(r'(\w*)(\w)\2(\w*)')
    repl = r'\1\2\3'
    if wordnet.synsets(word):
        return word
    repl_word = repeat_regexp.sub(repl, word)
    if repl_word != word:      
        return replaceElongated(repl_word)
    else:       
        return repl_word
def tokenize(text):
    finalTokens = []
    tokens = nltk.word_tokenize(text)
    for w in tokens:
        finalTokens.append(replaceElongated(w))
    text = " ".join(finalTokens)
    return remStopWords(text)

#Method:13 Removing stopwords
STOPWORDS = set(stopwords.words('english'))
def remStopWords(text):
    """custom function to remove the stopwords"""
    return lemmatizeWords(" ".join([word for word in str(text).split() if word not in STOPWORDS]))
#Method:14 Lemmatizing the words
lemmatizer = WordNetLemmatizer()
def lemmatizeWords(text):
    return correctSpelling(" ".join([lemmatizer.lemmatize(word) for word in text.split()]))

#Method:15 Replacing  spelling Mistakes 
    """def correctSpelling(text):
    for word in dic.keys():
        text = text.replace(word, dic[word])
    return text  """
#spell = SpellChecker()
def correctSpelling(text):

    """corrected_text = []
    s=text.split()
    misspelled_words = spell.unknown(s)
    for word in s:
        if word in misspelled_words:
            corrected_text.append(spell.correction(word))
        else:
            corrected_text.append(word)
    text=' '.join(corrected_text)"""
    
    return text
    
    
!pip install unidecode
    
from unidecode import unidecode
import unicodedata


print(df_train.isnull().sum())
print(df_test.isnull().sum())
df_train.fillna(" ")
df_test.fillna(" ")
print(df_train.isnull().sum())
print(df_test.isnull().sum())

x1=df_train['question1']
x2=df_train['question2']

print(len(x1))
for index, row in df_train.iterrows():
    x1[index] = frameWork(x1[index])
    x2[index] = frameWork(x2[index])
    print(index)
    

print(x1)

from sklearn.metrics.pairwise import cosine_similarity
import numpy; print("NumPy", numpy.__version__)
import scipy; print("SciPy", scipy.__version__)
import sklearn; print("Scikit-Learn", sklearn.__version__)
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, roc_curve, auc, roc_auc_score, 
                             confusion_matrix, classification_report)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import model_selection
from sklearn.model_selection import GridSearchCV
import pandas as pd
import io
import time
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
def BOW_vectorization():
    count_vectorizer = CountVectorizer(text)
    
    bow_text = count_vectorizer.fit_transform(text.values.astype('U'))
    bow_feature_names = count_vectorizer.get_feature_names()
    #visualizations().plot_frequency_distribution(bow_text, bow_feature_names)
    X = pd.DataFrame(bow_text.toarray(), columns = bow_feature_names)
    
    #visualizations().plot_TSNE_distribution(bow_text, Y)
    return X
def TFIDF_vectorization(text):
    tfidf_vectorizer = TfidfVectorizer(max_features = 5500)
    tfidf_text = tfidf_vectorizer.fit_transform(text.values.astype('U'))
    tfidf_feature_names = tfidf_vectorizer.get_feature_names()
    #visualizations().plot_frequency_distribution(tfidf_text, tfidf_feature_names)
    X = pd.DataFrame(tfidf_text.toarray(), columns = tfidf_feature_names)
    
    #visualizations().plot_TSNE_distribution(tfidf_text, Y)
    return X
X1=TFIDF_vectorization(x1)
X2=TFIDF_vectorization(x2)

#x=cosine_similarity(X1,X2, dense_output=True)

print(X1)
print(X2)
